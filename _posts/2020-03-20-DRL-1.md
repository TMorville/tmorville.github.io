---
title: Deep Contextual Bandits (1/3)
layout: single
author_profile: false
mathjax: true
---

### Deep Reinforcement Learning is gaining traction because of its ability to scale to problems that were previously intractable, such as learning to play Atari video games. This post is the first of three, where I explore a specific implementaion of a Deep Contextual Bandit, a kind of reinforcement learning algorithm. 

---

# WIP ⚠️

### Abstract

In the first post I go through the code and theory underlying the reinforcement learning and deep learning in some detail. I use this to reproduce the results from the article and produce some auxiliary visualisation in order to aid intuition. In the second post, I inject sparsity into the reward landscape and see how the agent performs. In the third - and last - post I experiment with different methods for improving the performance of the algorithm in the sparse setting. 

### Background



### The problem

### Reproducing basic results 

To get started we are going to use the jester data supplied [here](https://storage.googleapis.com/bandits_datasets/jester_data_40jokes_19181users.npy). This data is a subset of a [larger dataset](https://goldberg.berkeley.edu/jester-data/) containing only the users who rated all 40 jokes, which means it has no missing values. I have written [some code](https://github.com/TMorville/svd-rl) that adapts the jester data found in the deep contextual bandit repo. 

To summarise, the code does the following:

1. Reproduces the results from the paper on the Jester data using linear neural bandit. 
2. Run the same code on data where we increasingly and randomly obliterate rewards and replace them with zeros.
3. Apply Funk SVD to replace zeros and run the same code again. 
4. Explore results.

First, you will need to clone the repo

```bash
git clone git@github.com:TMorville/svd-rl.git
```

make a directory and and download the data

```python
mkdir data && cd data
wget https://storage.googleapis.com/bandits_datasets/jester_data_40jokes_19181users.npy
```

First, we will just reproduce the results from the repo. I've changed the code quite a bit to make it run isolated and added some graphics as well. You will find that most of the fundamental code remain the same, but the code that executes it is very different.

```python
import sys
import tensorflow as tf

sys.path.append('/path/to/repo')

DATA_PATH = '/path/to/data'

from src.sample_jester_data import sample_jester_data
from src.run_bandit import plot_model_actions, run_bandit
from src.linear_full_posterior_sampling import LinearFullPosteriorSampling
from src.neural_linear_sampling import NeuralLinearPosteriorSampling
```

Define the parameters for the neural bandit. The actions are the jokes the bandit will learn to recommend, and the context dimension is the size of the sampled data. You dont have to worry too much about the remaining parameters, but some of them are definitely of interest for fine-tuning later.

```python
hparams_nlinear = tf.contrib.training.HParams(num_actions=num_actions,
                                                context_dim=context_dim,
                                                init_scale=0.3,
                                                activation=tf.nn.relu,
                                                layer_sizes=[50],
                                                batch_size=512,
                                                activate_decay=True,
                                                initial_lr=0.1,
                                                max_grad_norm=5.0,
                                                show_training=False,
                                                freq_summary=1000,
                                                buffer_s=-1,
                                                initial_pulls=2,
                                                reset_lr=True,
                                                lr_decay_rate=0.5,
                                                training_freq=1,
                                                training_freq_network=50,
                                                training_epochs=100,
                                                a0=6,
                                                b0=6,
                                                lambda_prior=0.25,
                                                verbose=False)

neural_bandit = NeuralLinearPosteriorSampling('neural_bandit', hparams_nlinear)
```

Run the code and save the optimal action frequencies, the model action frequencies and the rewards. 

```python
oaf, maf, rewards, actions = run_bandit(model=neural_bandit, hparams=hparams_nlinear, num_contexts=2000, pct_zero=0.0, plot=True)
```

![1](https://i.imgur.com/fv7wr9Q.png)

The blue columns show the frequencies of optimal choice, our immutable ground truth. The red dynamic columns are the model choice frequencies. Notice that the red columns look uniformly distributed in the top left-hand figure. As the agent improves, the model actions approximates the optimal actions quite well.  

As you can imagine, being able to do this is extremely useful in many applications, not just recommendations. Effectively, we are seeing an algorithm that goes from ignorant to reproducing a discrete histogram in as little as 2000 iterations. Another interesting application of RL is to deep learning. Deep Reinforcement Learning (DRL) combines deep learning with reinforcement learning, allowing the latter to learn on the output from a DL model. [This survey](https://arxiv.org/abs/1708.05866) is a excellent summary of the recent advances.

### Obliterating rewards



### TODO

* Show decrease in performance when obliterating rewards
* Apply Funk SVD
* Use Chi-square goodness of fit to see if SVD improves results

