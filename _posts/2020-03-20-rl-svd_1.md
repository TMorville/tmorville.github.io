---
title: Can matrix decomposition improve performance of contextual bandits in a sparse environment? (1/2)
layout: single
author_profile: false
mathjax: true
---

### Sparse reward environments are commonplace in real world reinforcement learning problems. In this two-fold post, I explore matrix decomposition as a method for improving the performance of a multi-armed contextual bandit applied to recommendations.

---

# WIP ⚠️

### Abstract

This is the first part of a two-part node in which I explore the possibility of improving the performance of a multi-armed bandit in a environment with sparse rewards using matrix decomposition. 

In the first part I introduce the problem and apply the method to a data set where the the ground truth is known. In the second part of the post, I apply this method to a real world Ecommerce dataset. 

### Background

A multi-armed bandit is a simplified reinforcement learning (RL) problem, I wont go into too much detail, but I recommend reading [this post](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/) on the Unity blog to get good introduction to the basics. 

The purpose of this post is singular, and on the surface of it, well-defined. It can really be phrased in one sentence.

> *Can we use matrix decomposition to improve the performance of a multiarmed contextual bandit in a environment where rewards are sparse?*

This question became relevant as I was working on a solution that generates recommendations in a ecommerce webshop. However, I only had access to a small sample of anonymous data from production, so recommender systems based on collaborative filtering were irrelevant (see [cold start](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)) problem for more information). In this case, multi-armed bandits based on Thompson sampling have shown to perform really well. I found this [neat implementation](https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits) of deep contextual bandits by [Carlos Riquelme](http://rikel.me/) in the tensorflow research library (see paper here) and adapted the code, so I could run it on the sample data I had. 

Its commonplace for ecommerce data to look like its drawn from a [powerlaw distribution](https://en.wikipedia.org/wiki/Power_law). This means that a small amount of items make up the bulk of the sales, and a minority of users make up the vast majority of transactions. This was also the case for my data sample. This meant that the user-item or adjacency matrix became extremely sparse, and as a consequence, the RL algorithm had a difficult time mapping actions contingencies to rewards. This is an area of growing interest and there is some recent[ interesting work](https://arxiv.org/abs/1902.07198) coming out from Google Research on the subject.

The approach I am going to explore here is relatively simple. Use matrix decomposition by stochastic gradient descent optimisation - also known as [Funk SVD](https://sifter.org/~simon/journal/20061211.html) - to fill out missing values and use this to tune the parameters of our contextual bandit. 

In the face of it, its a halfway solution to both problems. If there was enough data, we could just use Funk SVD to generate recommendations. But we dont have enough data, so we will have to try something different, i.e. the contextual bandit. However, data is too sparse to produce any results. So, what if we combine the two? 

### Getting started

To get started we are going to use the jester data supplied [here](https://storage.googleapis.com/bandits_datasets/jester_data_40jokes_19181users.npy). This data is a subset of a [larger dataset](https://goldberg.berkeley.edu/jester-data/) containing only the users who rated all 40 jokes, which means it has no missing values. I have written [some code](https://github.com/TMorville/svd-rl) that adapts the jester data found in the deep contextual bandit repo. 

To summarise, the code does the following:

1. Reproduces the results from the paper on the Jester data using linear neural bandit. 
2. Run the same code on data where we increasingly and randomly obliterate rewards and replace them with zeros.
3. Apply Funk SVD to replace zeros and run the same code again. 
4. Explore results.

First, you will need to clone my repo

```bash
git clone git@github.com:TMorville/svd-rl.git
```

make a directory and and download the data

```python
mkdir data && cd data
wget https://storage.googleapis.com/bandits_datasets/jester_data_40jokes_19181users.npy
```

First, we will just reproduce the results from the repo. I've changed the code quite a bit to make it run isolated and added some graphics as well. You will find that most of the fundamental code remain the same, but the code that executes it is very different.

First, a few imports.

```python
import sys
import tensorflow as tf

sys.path.append('/path/to/repo')

DATA_PATH = '/path/to/data'

from src.sample_jester_data import sample_jester_data
from src.run_bandit import plot_model_actions, run_bandit
from src.linear_full_posterior_sampling import LinearFullPosteriorSampling
from src.neural_linear_sampling import NeuralLinearPosteriorSampling
```

Define the parameters for the neural bandit. The actions are the jokes the bandit will learn to recommend, and the context dimension is the size of the sampled data. You dont have to worry too much about the remaining parameters, but some of them are definitely of interest for fine-tuning later.

```python
hparams_nlinear = tf.contrib.training.HParams(num_actions=num_actions,
                                                context_dim=context_dim,
                                                init_scale=0.3,
                                                activation=tf.nn.relu,
                                                layer_sizes=[50],
                                                batch_size=512,
                                                activate_decay=True,
                                                initial_lr=0.1,
                                                max_grad_norm=5.0,
                                                show_training=False,
                                                freq_summary=1000,
                                                buffer_s=-1,
                                                initial_pulls=2,
                                                reset_lr=True,
                                                lr_decay_rate=0.5,
                                                training_freq=1,
                                                training_freq_network=50,
                                                training_epochs=100,
                                                a0=6,
                                                b0=6,
                                                lambda_prior=0.25,
                                                verbose=False)

neural_bandit = NeuralLinearPosteriorSampling('neural_bandit', hparams_nlinear)
```

Run the code and save the optimal action frequencies, the model action frequencies and the rewards. 

```python
oaf, maf, rewards, actions = run_bandit(model=neural_bandit, hparams=hparams_nlinear, num_contexts=500, pct_zero=0.0, plot=True)
```

![1](https://i.imgur.com/fv7wr9Q.png)

### TODO

* Show decrease in performance when obliterating rewards
* Apply Funk SVD
* Use Chi-square goodness of fit to see if SVD improves results