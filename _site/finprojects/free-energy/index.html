<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>A tutorial on the free-energy framework for modelling perception and learning -</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content=" ">
<meta property="og:title" content="A tutorial on the free-energy framework for modelling perception and learning">


  <link rel="canonical" href="https://tmorville.github.io//finprojects/free-energy/">
  <meta property="og:url" content="https://tmorville.github.io//finprojects/free-energy/">



  <meta property="og:description" content="Based on K. Friston and following R. Bogacz I shortly summarise the former, and solve the programming exercises given in the paper. This motives another post, taking a more general look at active learning in ML.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-03-27T15:43:03+02:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Tobias Morville",
      "url" : "https://tmorville.github.io/",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://tmorville.github.io//feed.xml" type="application/atom+xml" rel="alternate" title="  Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://tmorville.github.io//assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    <!-- start custom head snippets -->

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://tmorville.github.io//"> </a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tmorville.github.io/">home</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tmorville.github.io/writing/">writing</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tmorville.github.io/projects/">projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://tmorville.github.io/about/">about</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  



  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="A tutorial on the free-energy framework for modelling perception and learning">
    <meta itemprop="description" content="Based on K. Friston and following R. Bogacz I shortly summarise the former, and solve the programming exercises given in the paper. This motives another post, taking a more general look at active learning in ML.">
    <meta itemprop="datePublished" content="March 27, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">A tutorial on the free-energy framework for modelling perception and learning
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        <h3 id="based-on-k-friston-and-following-r-bogacz-i-shortly-summarise-the-former-and-solve-the-programming-exercises-given-in-the-paper-this-motives-another-post-taking-a-more-general-look-at-active-learning-in-ml">Based on <a href="http://www.fil.ion.ucl.ac.uk/~karl/A%20free%20energy%20principle%20for%20the%20brain.pdf">K. Friston</a> and following <a href="http://www.sciencedirect.com/science/article/pii/S0022249615000759">R. Bogacz</a> I shortly summarise the former, and solve the programming exercises given in the paper. This motives <a href="https://tmorville.github.io/projects/active-learning">another post</a>, taking a more general look at active learning in ML.</h3>

<hr>

<h3 id="why-is-this-importantexciting">Why is this important/exciting?</h3>

<p><a href="http://www.sciencedirect.com/science/article/pii/S0022249615000759">R. Bogacz</a> delivers a detailed and beautifully made tutorial on a subject that can be very difficult to understand, <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Bayes</a>, as seen from Karl Fristons perspective; Namely invoking the <a href="https://en.wikipedia.org/wiki/Free_energy_principle">Free-Energy principle</a> to motivate Active Inference. In the last decade, Active Inference has gained traction with the wider neuroscientific community, and recently Karl was <a href="https://www.ucl.ac.uk/news/slms/slms-news/slms/ucl-neuroscientists-most-influential">measured</a> to be the most influential neuroscientist in the modern era.</p>

<h3 id="nomenclature">Nomenclature:</h3>

<p>I use the same notation as in R. Bogacz. The code implements and refers to equations in the paper, and network graphs were provided by Bogacz himself.</p>

<hr>

<h3 id="introduction---biological-intuition">Introduction - Biological intuition</h3>
<p>A simple organism is trying to infer the size <script type="math/tex">v</script> of a food item. The only source of noisy information is one photoreceptor that signals the light reflected from this item, we denote this <script type="math/tex">u</script>. The non-linear function that relates size <script type="math/tex">v</script> to photosensory input <script type="math/tex">u</script> is assumed to be <script type="math/tex">g(v)=v^2</script>. We assume that this signal is normally distributed with mean <script type="math/tex">g(v)</script> and variance <script type="math/tex">\Sigma_{v}</script>.</p>

<hr>

<h3 id="part-i---bayes">Part I - Bayes</h3>

<p>We can write of the the likelihood function (probability of a size <script type="math/tex">v</script> given a signal <script type="math/tex">u</script>) as</p>

<script type="math/tex; mode=display">p(v\vert u)=f(u;g(v),\Sigma_{u})</script>

<p>where</p>

<script type="math/tex; mode=display">f(x;\mu,\Sigma)=\frac{1}{\sqrt{2\pi\Sigma}}\mbox{exp}\left(-\frac{(x-\mu)^{2}}{2\Sigma}\right)</script>

<p>is the normal distribution with mean <script type="math/tex">\mu</script> and variance <script type="math/tex">\Sigma</script>.</p>

<p>Through learning or evolutionary filtering, the agent has been endowed with priors on the expected size of food items, and therefore expects sizes of food items to normally distributed with mean <script type="math/tex">v_{p}</script> and <script type="math/tex">\Sigma_{p}</script> where the subscript <script type="math/tex">p</script> stands for prior. Formally</p>

<p><script type="math/tex">p(v)=f(v;v{p},\Sigma_{p})</script>.</p>

<p>To compute the exact distribution of sensory input <script type="math/tex">u</script> we can formulate the posterior using Bayes theorem</p>

<script type="math/tex; mode=display">p(v|u)=\frac{p(v)p(u|v)}{p(u)}</script>

<p>where the denominator is</p>

<script type="math/tex; mode=display">p(u)=\int p(v)p(u|v)dv</script>

<p>and sum the whole range of possible sizes.</p>

<p>The following code implements such an exact solution and plots it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s">"white"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s">"muted"</span><span class="p">,</span> <span class="n">color_codes</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># non-linear transformation of size to perceived sensory input e.g. g(phi)</span>
<span class="k">def</span> <span class="nf">sensory_transform</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        
    <span class="n">sensory_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">sensory_output</span>
</code></pre></div></div>

<p>The reason we explicitly define <script type="math/tex">g(\cdot)</script> is that we might want to change it later. For now we assume a simple non-linear relation <script type="math/tex">g(v)=v^2</script>. The following snippet of code assumes values of <script type="math/tex">v,\Sigma_u,v_p,\Sigma_p</script> and plots the posterior distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">exact_bayes</span><span class="p">():</span>
    
    <span class="c"># variabels </span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># real size of item</span>
    <span class="n">sigma_u</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># standard deviation of noisy input</span>
    <span class="n">v_p</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># mean of prior</span>
    <span class="n">sigma_p</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># variance of prior / sensory noise </span>
    <span class="n">s_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c"># range of sensory input</span>
    <span class="n">s_step</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># step size</span>
        
    <span class="c"># exact bayes (equation 4)</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">s_range</span><span class="p">,</span><span class="n">v_p</span><span class="p">,</span><span class="n">sigma_p</span><span class="p">),</span><span class="c"># prior</span>
                            <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="n">sensory_transform</span><span class="p">(</span><span class="n">s_range</span><span class="p">),</span><span class="n">sigma_u</span><span class="p">)))</span> <span class="c"># likelihood</span>
    <span class="n">normalisation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">numerator</span><span class="o">*</span><span class="n">s_step</span><span class="p">)</span> <span class="c"># denominator / model evidence / p(noisy input) (equation 5)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">normalisation</span> <span class="c"># posterior</span>
    
    <span class="c"># plot exact bayes</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">7.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s_range</span><span class="p">,</span><span class="n">posterior</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r' $p(v | u)$'</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">exact_bayes</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://tmorville.github.io//assets/images/Bogacz_5_0.png" alt="png"></p>

<p>Inspecting the graph, we find that approximately <script type="math/tex">\phi=1.6</script> maximises the posterior. There are two fundamental problems with this approach</p>

<ol>
  <li>
    <p>The posterior does not take a standard form, and is thus described by (potentially) infinitely many moments, instead of just simple sufficient statistics, such as the mean and the variance of a gaussian.</p>
  </li>
  <li>
    <p>The normalisation term that sits in the numerator of Bayes formula</p>
  </li>
</ol>

<script type="math/tex; mode=display">p(u)=\int p(v)p(u|v)dv</script>

<p>can be complicated and numerical solutions often rely on computationally intense algorithms, such as the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximisation algorithm</a>.</p>

<h3 id="part-ii---iteration-using-eulers-method">Part II - Iteration using Eulers method</h3>

<p>We are interested in a more general way of finding the value that maximises the posterior <script type="math/tex">\phi</script>. This involves maximising the numerator of Bayes equation. As this is independent of the denominator and therefore maximising <script type="math/tex">p(v)p(u\vert v)</script> will maximise the posterior. By taking the logarithm to the numerator we get</p>

<script type="math/tex; mode=display">F=\mbox{ln}p(\phi)+\mbox{ln}p(u|\phi)</script>

<p>and the dynamics can be derived (see notes) to be</p>

<script type="math/tex; mode=display">\dot{\phi}=\frac{v_{p}-\phi}{\Sigma_{p}}+\frac{u-g(\phi)}{\Sigma_{u}}g^{'}(\phi)</script>

<p>The next snippit of code asumes values for <script type="math/tex">v,\sigma_u,v_p,\sigma_p</script> and implements the above dynamics to find the value of <script type="math/tex">\phi</script> that maximises the posterior using a manual implementation of the dynamics and iterating using Eulers method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">simple_dyn</span><span class="p">():</span>
    
    <span class="c"># variabels </span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># real size of item</span>
    <span class="n">sigma_u</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># standard deviation of noisy input</span>
    <span class="n">v_p</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># mean of prior</span>
    <span class="n">sigma_p</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># variance of prior / sensory noise </span>
    <span class="n">s_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c"># range of sensory input</span>
    <span class="n">s_step</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># step size</span>
    
    <span class="c"># assume that phi maximises the posterior </span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">s_range</span><span class="p">))</span>
    
    <span class="c"># use Eulers method to find the most likely value of phi</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">s_range</span><span class="p">)):</span>
        
        <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_p</span>
        <span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">s_step</span> <span class="o">*</span> <span class="p">(</span> <span class="p">(</span> <span class="p">(</span><span class="n">v_p</span> <span class="o">-</span> <span class="n">phi</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">sigma_u</span> <span class="p">)</span> <span class="o">+</span>
        <span class="p">(</span> <span class="p">(</span> <span class="n">v</span> <span class="o">-</span> <span class="n">sensory_transform</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="p">)</span> <span class="o">/</span> <span class="n">sigma_u</span> <span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">phi</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="p">)</span> <span class="c"># equation 12</span>
    
    <span class="c"># plot convergence</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s_range</span><span class="p">,</span><span class="n">phi</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r' $\phi$'</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
    
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">simple_dyn</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://tmorville.github.io//assets/images/Bogacz_8_0.png" alt="png"></p>

<p>It is clear that the output converges rapidly to <script type="math/tex">\phi=1.6</script>, the value that maximises the posterior.</p>

<p>So we ask the question: What does a minimal and <em>biologically plausible</em> network model that can do such calculations look like?</p>

<hr>

<h3 id="part-iii---biological-plausibility">Part III - Biological plausibility</h3>

<p>Firstly, we must specify what exactly biologically plausible means.</p>

<ul>
  <li>
    <p><em>A neuron only performs computations on the input it is given, weighted by its synaptic weights.</em></p>
  </li>
  <li>
    <p><em>Synaptic plasticity of one neuron, is only based on the activity of pre-synaptic and post-synaptic activity connecting to that neuron.</em></p>
  </li>
</ul>

<p>Consider the dynamics of a simple network that relies on just two neurons and is coherent with the above requirements of local computation</p>

<script type="math/tex; mode=display">\dot{\epsilon_{p}} = \phi-v_{p}-\Sigma_{p}\epsilon_{p}</script>

<script type="math/tex; mode=display">\dot{\epsilon_{u}} = u-h(\phi)-\Sigma_{s}\epsilon_{s}</script>

<p>where <script type="math/tex">\epsilon_{p}</script> and <script type="math/tex">\epsilon_{s}</script> are the <em>prediction errors</em></p>

<script type="math/tex; mode=display">\epsilon_{p} = \frac{v_{p}-\phi}{\Sigma_{p}}</script>

<script type="math/tex; mode=display">\epsilon_{s} = \frac{s-g(\phi)}{\Sigma_{s}}</script>

<p>that arise from the assumption that the input is normally distributed (again, see Bogaez for derivations).</p>

<p>Below the network dynamics are sketched. Arrows denote excitatory input, circles inhibitory input,   and unless specified (<script type="math/tex">\theta,v_p,\Sigma_u,\Sigma_p</script>), weights of connections are assumed to be one.</p>

<p><img src="https://tmorville.github.io//assets/images/Bogacz_graph_1.png" alt="png"></p>

<p>The next snippit of code implements those dynamics and thus, the network “learns” what value of <script type="math/tex">\phi</script> that maximises the posterior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">learn_phi</span><span class="p">():</span>
    
    <span class="c"># variabels </span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># real size of item</span>
    <span class="n">sigma_u</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># standard deviation of noisy input</span>
    <span class="n">v_p</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># mean of prior</span>
    <span class="n">sigma_p</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># variance of prior / sensory noise </span>
    <span class="n">s_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c"># range of sensory input</span>
    <span class="n">s_step</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># step size</span>
    
    <span class="c"># preallocate</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">s_range</span><span class="p">))</span> 
    <span class="n">epsilon_e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">s_range</span><span class="p">))</span> 
    <span class="n">epsilon_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">s_range</span><span class="p">))</span>
    
    <span class="c"># dynamics of prediction errors for size (epsilon_v) and sensory input (epsilon_u)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">s_range</span><span class="p">)):</span>
        
        <span class="n">phi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_p</span> <span class="c"># initialise best guess (prior) of size</span>
        <span class="n">epsilon_e</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># initialise prediction error for size</span>
        <span class="n">epsilon_s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># initialise prediction error for sensory input</span>
        
        <span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">s_step</span><span class="o">*</span><span class="p">(</span> <span class="o">-</span><span class="n">epsilon_e</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">epsilon_s</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="p">)</span> <span class="p">)</span> <span class="c"># equation 12</span>
        <span class="n">epsilon_e</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">epsilon_e</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">s_step</span><span class="o">*</span><span class="p">(</span> <span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">v_p</span> <span class="o">-</span> <span class="n">sigma_u</span> <span class="o">*</span> <span class="n">epsilon_e</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span> <span class="c"># equation 13</span>
        <span class="n">epsilon_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">epsilon_s</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">s_step</span><span class="o">*</span><span class="p">(</span> <span class="n">v</span> <span class="o">-</span> <span class="n">sensory_transform</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">sigma_p</span> <span class="o">*</span> <span class="n">epsilon_s</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span> <span class="c"># equation 14</span>
    
    <span class="c"># plot network dynamics</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s_range</span><span class="p">,</span><span class="n">phi</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s_range</span><span class="p">,</span><span class="n">epsilon_e</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s_range</span><span class="p">,</span><span class="n">epsilon_s</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Activity'</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>                
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn_phi</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://tmorville.github.io//assets/images/Bogacz_11_0.png" alt="png"></p>

<p>As the figure shows, the network learns <span style="color: blue"><script type="math/tex">\phi</script></span> but is slower in converging than when using Eulers method, as the model relies on several other nodes that are inhibits and excites each other, which causes oscillatory behaviour. Both <span style="color:green"><script type="math/tex">\epsilon_p</script></span> and <span style="color:red"><script type="math/tex">\epsilon_v</script></span> oscillate and converge to the values where</p>

<script type="math/tex; mode=display">\epsilon_{p} \approx 0</script>

<script type="math/tex; mode=display">\epsilon_{s} \approx 0.</script>

<p>Which can be understood as the steady-state of the network. This means that minimising prediction errors (by learning the correct parameters) minimises the free energy potential - a tenet in Active Inference.</p>

<hr>

<h3 id="part-iv---approximate-estimation-of-sigma">Part IV - Approximate estimation of <script type="math/tex">\Sigma</script>
</h3>

<p>Recall that we assumed that size <script type="math/tex">v</script> was communicated via. a noisy signal <script type="math/tex">u</script> assumed to be normally distributed. Above, we outlined a simple sample method for finding the mean value <script type="math/tex">\phi</script> that maximises the posterior <script type="math/tex">p(v\vert s)</script>.</p>

<p>By expanding this simple model, we can esimate the variance <script type="math/tex">\Sigma</script> of the normal distribution as well. Considering computation in one single node computing prediction error</p>

<script type="math/tex; mode=display">\epsilon_{i}=\frac{\phi_{i}-g(\phi_{i+1})}{\Sigma_{i}}</script>

<p>where <script type="math/tex">\Sigma_{i}=\left\langle (\phi_{i}-g_{i}(\phi_{i+1})^{2}\right\rangle</script> is the variance of the best guess of <script type="math/tex">v</script>, <script type="math/tex">\phi_{i}</script>. Estimation of <script type="math/tex">\Sigma</script> can be achieved by adding a interneuron <script type="math/tex">e_{i}</script> which is connected to the prediction error node, and receives input from this via the connection with weight encoding <script type="math/tex">\Sigma_{i}</script>, such as sketched in the figure below.</p>

<p><img src="https://tmorville.github.io//assets/images/Bogacz_graph_2.png" alt="png" width="250px"></p>

<p>The dynamics are described by</p>

<script type="math/tex; mode=display">\dot{\epsilon_{i}} = \phi_{i}-g(\phi_{i+1})-e_{i}</script>

<script type="math/tex; mode=display">\dot{e} = \Sigma_{i}\epsilon_{i}-e_{i}</script>

<p>which the following snippit of code implements.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">learn_sigma</span><span class="p">():</span>
    
    <span class="c"># variabels </span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># observed homeostatic error </span>
    <span class="n">sigma_u</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># standard deviation of the homeostatic error</span>
    <span class="n">v_p</span> <span class="o">=</span> <span class="mi">3</span> <span class="c"># mean of prior homeostatic error / noisy input / simple prior</span>
    <span class="n">sigma_p</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># variance of prior / sensory noise </span>
    <span class="n">s_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span> <span class="c"># range of sensory input</span>
    <span class="n">s_step</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># step size</span>
    
    <span class="c"># new variabels </span>
    <span class="n">maxt</span> <span class="o">=</span> <span class="mi">20</span> <span class="c"># maximum number of iterations</span>
    <span class="n">trials</span> <span class="o">=</span> <span class="mi">2000</span> <span class="c"># of trials </span>
    <span class="n">epi_length</span> <span class="o">=</span> <span class="mi">20</span> <span class="c"># length of episode</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c"># learning rate</span>
    
    <span class="n">mean_phi</span> <span class="o">=</span> <span class="mi">5</span> <span class="c"># the average value that maximises the posterior</span>
    <span class="n">sigma_phi</span> <span class="o">=</span> <span class="mi">2</span> <span class="c"># the variance of phi</span>
    <span class="n">last_phi</span> <span class="o">=</span> <span class="mi">5</span> <span class="c"># the last observed phi</span>
    
    <span class="c"># preallocate</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">trials</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">trials</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">trials</span><span class="p">)</span>
    
    <span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># initialise sigma in 1 </span>
    
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">trials</span><span class="p">):</span>
        
        <span class="n">error</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># initialise error in zero</span>
        <span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># initialise interneuron e in zero</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="c"># draw a new phi every round </span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2000</span><span class="p">):</span>
            
            <span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">s_step</span><span class="o">*</span><span class="p">(</span><span class="n">phi</span><span class="o">-</span><span class="n">last_phi</span><span class="o">-</span><span class="n">e</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c"># equation 59</span>
            <span class="n">e</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">e</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">s_step</span><span class="o">*</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">e</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c"># equation 60 </span>
            
        <span class="n">sigma</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">[</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">error</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">e</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c"># synaptic weight (Sigma) update</span>
        
    <span class="c"># plot dynamics of Sigma</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r' $\Sigma$'</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>           
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn_sigma</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://tmorville.github.io//assets/images/Bogacz_14_0.png" alt="png"></p>

<p>Because <script type="math/tex">\phi</script> is constantly varying <code>phi = np.random.normal(5, np.sqrt(2), 1)</code> <script type="math/tex">\Sigma</script> never converge to just one value, but instead to <em>approximately</em> 2, the variance of <script type="math/tex">\phi_i</script>.</p>

        
      </section>

      <footer class="page__meta">
        
        




        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-03-27T15:43:03+02:00">March 27, 2018</time></p>
        
      </footer>

      

      


  <nav class="pagination">
    
      <a href="https://tmorville.github.io//finprojects/coin-game/" class="pagination--pager" title="Non-ergodicity in a simple coin game
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    <script src="https://tmorville.github.io//assets/js/main.min.js"></script>




  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-89851834-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>






  </body>
</html>
