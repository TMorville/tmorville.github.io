---
title: Deep Contextual Bandits (2/3)
layout: single
author_profile: false
mathjax: true
---

### Sparse reward environments are commonplace in real world reinforcement learning problems. In this second part of a three-part note on Deep Contextual Bandits, I explore what happens to the performance of the bandit when its faced with a environment in which rewards are sparse. 

---

# WIP ⚠️

### Abstract 

In this post, I inject sparsity into the reward landscape and see how the agent performs. I also apply the solution to a sparse real world ecommerce dataset with the intention of generating recommendations. 

### Background

A multi-armed bandit is a simplified reinforcement learning (RL) problem, I wont go into too much detail, but I recommend reading [this post](https://blogs.unity3d.com/2017/06/26/unity-ai-themed-blog-entries/) on the Unity blog to get good introduction to the basics. 

The purpose of this post is singular, and on the surface of it, well-defined. It can really be phrased in one sentence.

> *Can we use matrix decomposition to improve the performance of a multiarmed contextual bandit in a environment where rewards are sparse?*

This question became relevant as I was working on a solution that generates recommendations in a ecommerce webshop. However, I only had access to a small sample of anonymous data from production, so recommender systems based on collaborative filtering were irrelevant (see [cold start](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)) problem for more information). In this case, multi-armed bandits based on Thompson sampling have shown to perform really well. I found this [neat implementation](https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits) of deep contextual bandits by [Carlos Riquelme](http://rikel.me/) in the tensorflow research library (see paper here) and adapted the code, so I could run it on the sample data I had. 

### The problem

Its commonplace for ecommerce data to look like its drawn from a [powerlaw distribution](https://en.wikipedia.org/wiki/Power_law). This means that a small amount of items make up the bulk of the sales, and a minority of users make up the vast majority of transactions. This means that the user-item, or adjacency, matrix becomes exceedingly sparse, and as a consequence, the RL algorithm has a difficult time mapping actions contingencies to rewards. This is an area of growing interest and there is some recent[ interesting work](https://arxiv.org/abs/1902.07198) coming out from Google Research on the subject.

<details>     
  <summary> Infobox: Sparsity
  </summary> The number of zero-valued elements divided by the total number of elements.
</details>

The approach I am going to explore here is relatively simple. Use matrix decomposition by stochastic gradient descent optimisation - also known as [Funk SVD](https://sifter.org/~simon/journal/20061211.html) - to fill out missing values and use this to tune the parameters of our contextual bandit. 

